{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build your own neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is all up to you to build your own neural network from scratch and apply a hyperparameter search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('cars.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.fillna(0)\n",
    "\n",
    "y = df[['price']]\n",
    "X = df.drop(['price'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in X.columns:\n",
    "    if X[column].dtype == np.object:\n",
    "        print('Converting ', column)\n",
    "        X = pd.concat([X,pd.get_dummies(X[column], prefix=column, drop_first=True)],axis=1).drop([column],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating train and test set and standardising:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "X_train = StandardScaler().fit_transform(X_train)\n",
    "y_train = StandardScaler().fit_transform(y_train)\n",
    "\n",
    "X_test = StandardScaler().fit(X_train).transform(X_test)\n",
    "y_test = StandardScaler().fit(y_train).transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline models: linear regression and random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train,np.ravel(y_train))\n",
    "prediction = lr.predict(X_test)\n",
    "\n",
    "print('RMSE linear regression:', np.sqrt(mse(y_test,prediction)))\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100)\n",
    "rf.fit(X_train,np.ravel(y_train))\n",
    "prediction = rf.predict(X_test)\n",
    "\n",
    "print('RMSE random forest:', np.sqrt(mse(y_test,prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incorporate the following:\n",
    "- 2 different kernels in hidden layer.\n",
    "- 2 different output kernels.\n",
    "- Different sizes for the hidden layers.\n",
    "- Different number of hidden layers.\n",
    "- Use 10 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, create your model compatible with Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def nn_model(no_neurons,no_layers,kernel,output_kernel):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(no_neurons,input_dim=input_dim))\n",
    "    model.add(Activation(kernel))\n",
    "\n",
    "    # Extra hidden layers\n",
    "    for _ in range(0,no_layers):\n",
    "        model.add(Dense(no_neurons))\n",
    "        model.add(Activation(kernel))\n",
    "\n",
    "    # Output\n",
    "    model.add(Dense(output_dim))\n",
    "    model.add(Activation(output_kernel))\n",
    "    model.compile(optimizer=Adam(),loss='mean_squared_error',metrics=['mean_squared_error'])\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, run your grid search (10 epochs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "input_dim = X.shape[1]\n",
    "output_dim = 1\n",
    "\n",
    "parameters = {'no_neurons':[50,100],'kernel':['relu','linear'],'output_kernel':['linear','sigmoid'],'no_layers':[1,2],'verbose':[0]} \n",
    "\n",
    "grid_search = GridSearchCV(KerasRegressor(nn_model), parameters, cv=5,scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X, y,epochs=10)\n",
    "\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "\n",
    "print('Mean RMSE (+/- standard deviation), for parameters')\n",
    "for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n",
    "    print(\"%0.3f (+/- %0.03f) for %r\"\n",
    "          # The MSE is return as a negative, so we multiple it with -1 before squaring it\n",
    "          % (np.sqrt(-1*mean), np.sqrt(std), params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that there is quite some difference between using the sigmiod and linear output kernel. The former gives a lower RMSE. The better results are obtained using ReLU as the activation function in the hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smaller hidden layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "input_dim = X.shape[1]\n",
    "output_dim = 1\n",
    "\n",
    "parameters = {'no_neurons':[10,20],'kernel':['relu','linear'],'output_kernel':['linear','sigmoid'],'no_layers':[0,1],'verbose':[0]} \n",
    "\n",
    "grid_search = GridSearchCV(KerasClassifier(nn_model), parameters, cv=5,scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X, y,epochs=10)\n",
    "\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "\n",
    "print('Mean RMSE (+/- standard deviation), for parameters')\n",
    "for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n",
    "    print(\"%0.3f (+/- %0.03f) for %r\"\n",
    "          # The MSE is return as a negative, so we multiple it with -1 before squaring it\n",
    "          % (np.sqrt(-1*mean), np.sqrt(std), params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clearly have a higher error rate, meaning that a bigger network pays off in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try more epochs for the smaller hidden layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "input_dim = X.shape[1]\n",
    "output_dim = 1\n",
    "\n",
    "parameters = {'no_neurons':[10,20],'kernel':['relu','linear'],'output_kernel':['linear','sigmoid'],'no_layers':[0,1],'verbose':[0]} \n",
    "\n",
    "grid_search = GridSearchCV(KerasClassifier(nn_model), parameters, cv=5,scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X, y,epochs=100)\n",
    "\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "\n",
    "print('Mean RMSE (+/- standard deviation), for parameters')\n",
    "for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n",
    "    print(\"%0.3f (+/- %0.03f) for %r\"\n",
    "          # The MSE is return as a negative, so we multiple it with -1 before squaring it\n",
    "          % (np.sqrt(-1*mean), np.sqrt(std), params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No difference here except for the reLU kernel based ones. we might be overfitting in the other cases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
