{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building neural networks for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have focused on classification into two classes with neural networks. Now, we will look at the use of neural networks for regression problems. We're going back to our diabetes progression dataset that we've seen in our regression tree building. We will also talk about learning rate - one of the parameters that you can choose in building your network. The learning rate refers to how strongly the model is changed during each of the weights/biases update step of backpropagation. We'll talk about it more later in this script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll already be familiar with this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### added line to ensure plots are showing\n",
    "%matplotlib inline\n",
    "#####\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "dataset = load_diabetes()\n",
    "\n",
    "X = pd.DataFrame(data=dataset['data'],columns=dataset['feature_names'])\n",
    "\n",
    "y = pd.DataFrame(data=dataset['target'],columns=['progression'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a neural network regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we prepare our training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "# Scale the training and the test data\n",
    "X_train = StandardScaler().fit_transform(X_train)\n",
    "X_test = StandardScaler().fit(X_train).transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we obtain a metric that can be used for regression, the mean squared error, which is also used for the loss function. You'll remember this metric as the equivalent to, for example, cross entropy which is used for classification tasks.\n",
    "\n",
    "We now also use a different optimiser (instead of stochastic gradient descent) which works better in this instance and run 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 12:07:00.013455: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-01 12:07:00.153615: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-01 12:07:00.153653: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-01 12:07:00.865968: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-01 12:07:00.866082: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-01 12:07:00.866093: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/opt/conda/lib/python3.9/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 50)                550       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 601\n",
      "Trainable params: 601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 12:07:01.991470: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-12-01 12:07:01.991501: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-01 12:07:01.991522: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (noteable): /proc/driver/nvidia/version does not exist\n",
      "2022-12-01 12:07:01.991756: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 2ms/step - loss: 29554.6855 - mean_squared_error: 29554.6855\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 29485.7988 - mean_squared_error: 29485.7988\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 29409.0195 - mean_squared_error: 29409.0195\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 29339.9805 - mean_squared_error: 29339.9805\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 29265.2656 - mean_squared_error: 29265.2656\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 29184.0352 - mean_squared_error: 29184.0352\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 29094.3008 - mean_squared_error: 29094.3008\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 29000.2559 - mean_squared_error: 29000.2559\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 28904.9297 - mean_squared_error: 28904.9297\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 28791.0137 - mean_squared_error: 28791.0137\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "RMSE: 166.0361824163538\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam, SGD\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "# We only have 1 output dimension, as our regression outputs a real number\n",
    "output_dim = 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(50,input_dim=input_dim))\n",
    "\n",
    "model.add(Dense(output_dim))\n",
    "\n",
    "# We now use a dedicated optimizer instance - this allows us to input the learning rate later\n",
    "model.compile(optimizer=Adam(),loss='mean_squared_error',metrics=['mean_squared_error'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# We add the number of epochs as a parameter to our fit method\n",
    "model.fit(X_train,y_train,epochs=10)\n",
    "\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "print('RMSE:', np.sqrt(mse(y_test,prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how our mean squared error decreases over the 10 epochs that we're running as the model is learning with each runthrough of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will try to do a small hyperparameter optimisation exercise where we try to find the best regression model by altering:\n",
    "- The number of neurons in the hidden layer\n",
    "- The activation function\n",
    "- The learning rate\n",
    "- The number of epochs\n",
    "\n",
    "From these four we've not discussed the learning rate in a lot of detail so far. But it is one of the more important parameters in tuning your model. Think back to how backpropagation works - we're adjusting weights and biases according to the \"wishes\" of our training data. How strongly we actually change weights and biases in each of the backpropagation runs is referred to as the learning rate. In the plotted gradient descent example from class you can think about this as the step size that we take down the hill in our search of the minimum.\n",
    "\n",
    "A large learning rate means that we adapt quicker to our problem. But it also means that we might converge too quickly and not find an optimal solution. Think about this as big steps down the hill.\n",
    "A small learning rate means that we take greater care in finding our best solution. But small steps take a lot of time. Think about this as small steps down the hill.\n",
    "\n",
    "We'll now try to optimise the learning rate as well as some other hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use ```GridSearchCV``` from scikit-learn. However, we need to make an instance of a neural network we can feed to the grid search. Hence, we first create a neural network with the hyperparameters as inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def nn_model(no_neurons,learning_rate,kernel='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(no_neurons,input_dim=X_train.shape[1]))\n",
    "    model.add(Activation(kernel))\n",
    "\n",
    "    # Extra hidden layer\n",
    "    model.add(Dense(no_neurons))\n",
    "    model.add(Activation(kernel))\n",
    "\n",
    "    # Output\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    # Here, we can add the learning rate to the optimiser\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),loss='mean_squared_error',metrics=['mean_squared_error'])\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we add that model to our grid search as follows. Notice also how we setup the parameters to match the inputs of the model we just created.\n",
    "\n",
    "Warning: the next cell may take a few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_236/932712346.py:8: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  grid_search = GridSearchCV(KerasClassifier(nn_model), parameters, cv=5,scoring='neg_mean_squared_error')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f6ee2c2a790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:6 out of the last 14 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f6ee2a84dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Mean RMSE (+/- standard deviation), for parameters\n",
      "146.710 (+/- 55.127) for {'epochs': 5, 'kernel': 'relu', 'learning_rate': 0.0001, 'no_neurons': 5, 'verbose': 0}\n",
      "147.321 (+/- 55.991) for {'epochs': 5, 'kernel': 'relu', 'learning_rate': 0.0001, 'no_neurons': 20, 'verbose': 0}\n",
      "142.932 (+/- 54.369) for {'epochs': 5, 'kernel': 'relu', 'learning_rate': 0.01, 'no_neurons': 5, 'verbose': 0}\n",
      "142.932 (+/- 54.369) for {'epochs': 5, 'kernel': 'relu', 'learning_rate': 0.01, 'no_neurons': 20, 'verbose': 0}\n",
      "146.197 (+/- 54.962) for {'epochs': 5, 'kernel': 'linear', 'learning_rate': 0.0001, 'no_neurons': 5, 'verbose': 0}\n",
      "146.250 (+/- 56.566) for {'epochs': 5, 'kernel': 'linear', 'learning_rate': 0.0001, 'no_neurons': 20, 'verbose': 0}\n",
      "142.960 (+/- 54.367) for {'epochs': 5, 'kernel': 'linear', 'learning_rate': 0.01, 'no_neurons': 5, 'verbose': 0}\n",
      "142.932 (+/- 54.369) for {'epochs': 5, 'kernel': 'linear', 'learning_rate': 0.01, 'no_neurons': 20, 'verbose': 0}\n",
      "145.902 (+/- 54.501) for {'epochs': 10, 'kernel': 'relu', 'learning_rate': 0.0001, 'no_neurons': 5, 'verbose': 0}\n",
      "146.667 (+/- 57.532) for {'epochs': 10, 'kernel': 'relu', 'learning_rate': 0.0001, 'no_neurons': 20, 'verbose': 0}\n",
      "142.932 (+/- 54.369) for {'epochs': 10, 'kernel': 'relu', 'learning_rate': 0.01, 'no_neurons': 5, 'verbose': 0}\n",
      "142.932 (+/- 54.369) for {'epochs': 10, 'kernel': 'relu', 'learning_rate': 0.01, 'no_neurons': 20, 'verbose': 0}\n",
      "145.684 (+/- 56.630) for {'epochs': 10, 'kernel': 'linear', 'learning_rate': 0.0001, 'no_neurons': 5, 'verbose': 0}\n",
      "145.526 (+/- 55.252) for {'epochs': 10, 'kernel': 'linear', 'learning_rate': 0.0001, 'no_neurons': 20, 'verbose': 0}\n",
      "142.932 (+/- 54.369) for {'epochs': 10, 'kernel': 'linear', 'learning_rate': 0.01, 'no_neurons': 5, 'verbose': 0}\n",
      "142.932 (+/- 54.369) for {'epochs': 10, 'kernel': 'linear', 'learning_rate': 0.01, 'no_neurons': 20, 'verbose': 0}\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# We create a dictionary again, with keys matching our neural network function we create above \n",
    "parameters = {'no_neurons':[5,20],'kernel':['relu','linear'],'learning_rate':[0.0001,0.01],'epochs':[5,10],'verbose':[0]} \n",
    "\n",
    "# We wrap our model into KerasClassifier to bridge the gap between scikit-learn and Keras\n",
    "grid_search = GridSearchCV(KerasClassifier(nn_model), parameters, cv=5,scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "\n",
    "print('Mean RMSE (+/- standard deviation), for parameters')\n",
    "for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n",
    "    print(\"%0.3f (+/- %0.03f) for %r\"\n",
    "          # The MSE is return as a negative, so we multiple it with -1 before squaring it\n",
    "          % (np.sqrt(-1*mean), np.sqrt(std), params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scroll all the way down and you'll see the error rate that is achieved with different combinations of number of epochs/kernel/learning rate/number of neurons.\n",
    "\n",
    "It seems there is very little difference in terms of RMSE. We cannot say which hyperparameters are working better than others. Perhaps we should do a wider search, but this takes even more time. Later on, we will see how different results can be given the hyperparameters.\n",
    "A good hyperparameter search can result in very different networks more or less suitable for the data at hand."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
