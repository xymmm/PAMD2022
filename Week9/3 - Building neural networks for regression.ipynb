{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building neural networks for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this activity we will build a neural network for regression for the medical data set on diabetes progression. Furthermore, we will introduce the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same dataset as before. We can shorten our code a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### added line to ensure plots are showing\n",
    "%matplotlib inline\n",
    "#####\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "dataset = load_diabetes()\n",
    "\n",
    "X = pd.DataFrame(data=dataset['data'],columns=dataset['feature_names'])\n",
    "\n",
    "y = pd.DataFrame(data=dataset['target'],columns=['progression'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a neural network regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we prepare our training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "# Scale the training and the test data\n",
    "X_train = StandardScaler().fit_transform(X_train)\n",
    "X_test = StandardScaler().fit(X_train).transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we obtain a metric that can be used for regression, the mean squared error, which is also used for the loss function. We now use a different optimiser (instead of stochastic gradient descent) which works better in this instance and run 10 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam, SGD\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "# We only have 1 output dimension, as our regression output a real number\n",
    "output_dim = 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(50,input_dim=input_dim))\n",
    "\n",
    "model.add(Dense(output_dim))\n",
    "\n",
    "# We now use a dedicated optimizer instance - this allows us to input the learning rate later\n",
    "model.compile(optimizer=Adam(),loss='mean_squared_error',metrics=['mean_squared_error'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# We add the number of epochs as a parameter to our fit method\n",
    "model.fit(X_train,y_train,epochs=10)\n",
    "\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "print('RMSE:', np.sqrt(mse(y_test,prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will try to do a small hyperparameter optimisation exercise where we try to find the best regression model by altering:\n",
    "- The number of neurons in the hidden layer\n",
    "- The activation function\n",
    "- The learning rate\n",
    "- The number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use ```GridSearchCV``` from scikit-learn. However, we need to make an instance of a neural network we can feed to the grid search. Hence, we first create a neural network with the hyperparameters as inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def nn_model(no_neurons,learning_rate,kernel='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(no_neurons,input_dim=X_train.shape[1]))\n",
    "    model.add(Activation(kernel))\n",
    "\n",
    "    # Extra hidden layer\n",
    "    model.add(Dense(no_neurons))\n",
    "    model.add(Activation(kernel))\n",
    "\n",
    "    # Output\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    # Here, we can add the learning rate to the optimiser\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),loss='mean_squared_error',metrics=['mean_squared_error'])\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we add that model to our grid search as follows. Notice also how we setup the parameters to match the inputs of the model we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# We create a dictionary again, with keys matching our neural network function we create above \n",
    "parameters = {'no_neurons':[5,20],'kernel':['relu','linear'],'learning_rate':[0.0001,0.01],'epochs':[5,10],'verbose':[0]} \n",
    "\n",
    "# We wrap our model into KerasClassifier to bridge the gap between scikit-learn and Keras\n",
    "grid_search = GridSearchCV(KerasClassifier(nn_model), parameters, cv=5,scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "\n",
    "print('Mean RMSE (+/- standard deviation), for parameters')\n",
    "for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n",
    "    print(\"%0.3f (+/- %0.03f) for %r\"\n",
    "          # The MSE is return as a negative, so we multiple it with -1 before squaring it\n",
    "          % (np.sqrt(-1*mean), np.sqrt(std), params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems there is very little difference in terms of RMSE. We cannot say which hyperparameters are working better than others. Perhaps we should do a wider search, but this takes even more time. Later on, we will see how different results can be given the hyperparameters.\n",
    "A good hyperparameter search can result in very different networks more or less suitable for the data at hand."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
