{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to model the circles dataset again, and see how not only the kernel, but all hyperparameters affect our outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_circles\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y = make_circles(n_samples=1000, noise=0.05)\n",
    "\n",
    "df = pd.DataFrame(dict(x=X[:,0], y=X[:,1], label=y))\n",
    "colors = {0:'red', 1:'blue'}\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create our training and test sets again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run the same model with linear kernel as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(50,input_dim=input_dim))\n",
    "model.add(Activation('linear'))\n",
    "model.add(Dense(50))\n",
    "model.add(Activation('linear'))\n",
    "model.add(Dense(output_dim))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(optimizer='sgd',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit(X_train,y_train,epochs=10)\n",
    "\n",
    "prediction_prob = model.predict(X_test)\n",
    "prediction_class = (prediction_prob > 0.5).astype(\"int32\")\n",
    "\n",
    "\n",
    "print('Accuracy:', accuracy_score(y_test,prediction_class))\n",
    "print('AUC:',roc_auc_score(y_test,prediction_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not very good is it? We knew this was coming from playing around in the TensorFlow playground and our earlier experiments. We can visualise the result as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dict(x=X_test[:,0], y=X_test[:,1], label=prediction_class[:,0]))\n",
    "colors = {0:'red', 1:'blue'}\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear kernel is clearly not suitable. Let's try something different:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore a range of hyperparameters. More specifically, we will focus on:\n",
    "- Activation function\n",
    "- Number of neurons in a hidden layer\n",
    "- Number of layers\n",
    "- Learning rate\n",
    "- Number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def nn_model(no_neurons,learning_rate,no_layers,kernel):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(no_neurons,input_dim=input_dim))\n",
    "    model.add(Activation(kernel))\n",
    "\n",
    "    # Extra hidden layers\n",
    "    for _ in range(0,no_layers):\n",
    "        model.add(Dense(no_neurons))\n",
    "        model.add(Activation(kernel))\n",
    "\n",
    "    # Output\n",
    "    model.add(Dense(output_dim))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we run the grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'no_neurons':[50,100],'kernel':['relu','sigmoid','linear'],'no_layers':[1,2],'learning_rate':[0.1,0.01,0.001],'epochs':[10,20],'verbose':[0]} \n",
    "\n",
    "grid_search = GridSearchCV(KerasClassifier(nn_model), parameters, cv=5,scoring='roc_auc')\n",
    "grid_search.fit(X_train, y_train.ravel())\n",
    "\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "\n",
    "print('Mean AUC (+/- standard deviation), for parameters')\n",
    "for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n",
    "    print(\"%0.3f (+/- %0.03f) for %r\"\n",
    "          % (mean, std, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we see that the ReLU kernel clearly outperforms the linear and sigmoid one (although the sigmoid one is slightly higher and above 50%). Besides, we notice that the influence of epochs is apparent: results appear to be much more homogeneous over the various learning rates. The middle learning rate (0.01), however, tends to result in the highest AUC. The structure of the network (number of layers and number of neurons) seems to have little impact on the final results.\n",
    "This shows how effective hyperparameter search can be in order to finetune your neural network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
