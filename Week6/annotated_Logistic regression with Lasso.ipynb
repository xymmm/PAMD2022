{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mineral-repair",
   "metadata": {},
   "source": [
    "# Penalized (Lasso) logistic model\n",
    "\n",
    "Remember with logistic regression, we are modelling an event occurrence e.g. someone defaults on their credit, someone buys a product etc.\n",
    "\n",
    "We can also use logistic regression in a multi-class problem.\n",
    "\n",
    "Let's illustrate that using the Wine data set:\n",
    "\n",
    "https://scikit-learn.org/stable/datasets/toy_dataset.html#wine-dataset\n",
    "\n",
    "This dataset consists of 178 instances and 13 variables, describing different wines and their characteristics. The dataset also gives us three classes\n",
    "\n",
    "class_0\n",
    "class_1\n",
    "class_2\n",
    "\n",
    "which we want to predict.\n",
    "\n",
    "The class distribution is relatively evenly spread:\n",
    "\n",
    "class_0 (59), class_1 (71), class_2 (48)\n",
    "\n",
    "Remember that in some instances, when you're trying to predict particularly rare classes, you might need to do some pre-processing of the data (e.g. through oversampling). Otherwise, you run into the risk of the predictive model not being able to fit properly to those classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "stainless-david",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by importing today's packages\n",
    "\n",
    "import sklearn.datasets as datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "\n",
    "# Load the dataset and specify which part will be the predictors and which the to-be-predicted outcome\n",
    "dataset = datasets.load_wine()\n",
    "X = dataset['data']\n",
    "y = dataset['target']\n",
    "\n",
    "# Split data, here 70/30 split\n",
    "x_train, x_test, y_train, y_test = tts(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fc70d0",
   "metadata": {},
   "source": [
    "Remember that logistic regression (just like linear regression last week) is sensitive towards scale differences. We therefore scale our predictors. Output variables don't usually need scaling, as we are not making direct comparisons among them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff706e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled predictors\n",
    "x_train_scaled = StandardScaler().fit_transform(x_train)\n",
    "x_test_scaled = StandardScaler().fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "minor-stanford",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>13.000618</td>\n",
       "      <td>2.336348</td>\n",
       "      <td>2.366517</td>\n",
       "      <td>19.494944</td>\n",
       "      <td>99.741573</td>\n",
       "      <td>2.295112</td>\n",
       "      <td>2.029270</td>\n",
       "      <td>0.361854</td>\n",
       "      <td>1.590899</td>\n",
       "      <td>5.058090</td>\n",
       "      <td>0.957449</td>\n",
       "      <td>2.611685</td>\n",
       "      <td>746.893258</td>\n",
       "      <td>0.938202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.811827</td>\n",
       "      <td>1.117146</td>\n",
       "      <td>0.274344</td>\n",
       "      <td>3.339564</td>\n",
       "      <td>14.282484</td>\n",
       "      <td>0.625851</td>\n",
       "      <td>0.998859</td>\n",
       "      <td>0.124453</td>\n",
       "      <td>0.572359</td>\n",
       "      <td>2.318286</td>\n",
       "      <td>0.228572</td>\n",
       "      <td>0.709990</td>\n",
       "      <td>314.907474</td>\n",
       "      <td>0.775035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>11.030000</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>1.360000</td>\n",
       "      <td>10.600000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>1.280000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>1.270000</td>\n",
       "      <td>278.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>12.362500</td>\n",
       "      <td>1.602500</td>\n",
       "      <td>2.210000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>1.742500</td>\n",
       "      <td>1.205000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>3.220000</td>\n",
       "      <td>0.782500</td>\n",
       "      <td>1.937500</td>\n",
       "      <td>500.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.050000</td>\n",
       "      <td>1.865000</td>\n",
       "      <td>2.360000</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>2.355000</td>\n",
       "      <td>2.135000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>1.555000</td>\n",
       "      <td>4.690000</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>2.780000</td>\n",
       "      <td>673.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>13.677500</td>\n",
       "      <td>3.082500</td>\n",
       "      <td>2.557500</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>2.875000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>1.950000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>1.120000</td>\n",
       "      <td>3.170000</td>\n",
       "      <td>985.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14.830000</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.230000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>3.880000</td>\n",
       "      <td>5.080000</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>3.580000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1.710000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1680.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          alcohol  malic_acid         ash  alcalinity_of_ash   magnesium  \\\n",
       "count  178.000000  178.000000  178.000000         178.000000  178.000000   \n",
       "mean    13.000618    2.336348    2.366517          19.494944   99.741573   \n",
       "std      0.811827    1.117146    0.274344           3.339564   14.282484   \n",
       "min     11.030000    0.740000    1.360000          10.600000   70.000000   \n",
       "25%     12.362500    1.602500    2.210000          17.200000   88.000000   \n",
       "50%     13.050000    1.865000    2.360000          19.500000   98.000000   \n",
       "75%     13.677500    3.082500    2.557500          21.500000  107.000000   \n",
       "max     14.830000    5.800000    3.230000          30.000000  162.000000   \n",
       "\n",
       "       total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\n",
       "count     178.000000  178.000000            178.000000       178.000000   \n",
       "mean        2.295112    2.029270              0.361854         1.590899   \n",
       "std         0.625851    0.998859              0.124453         0.572359   \n",
       "min         0.980000    0.340000              0.130000         0.410000   \n",
       "25%         1.742500    1.205000              0.270000         1.250000   \n",
       "50%         2.355000    2.135000              0.340000         1.555000   \n",
       "75%         2.800000    2.875000              0.437500         1.950000   \n",
       "max         3.880000    5.080000              0.660000         3.580000   \n",
       "\n",
       "       color_intensity         hue  od280/od315_of_diluted_wines      proline  \\\n",
       "count       178.000000  178.000000                    178.000000   178.000000   \n",
       "mean          5.058090    0.957449                      2.611685   746.893258   \n",
       "std           2.318286    0.228572                      0.709990   314.907474   \n",
       "min           1.280000    0.480000                      1.270000   278.000000   \n",
       "25%           3.220000    0.782500                      1.937500   500.500000   \n",
       "50%           4.690000    0.965000                      2.780000   673.500000   \n",
       "75%           6.200000    1.120000                      3.170000   985.000000   \n",
       "max          13.000000    1.710000                      4.000000  1680.000000   \n",
       "\n",
       "           target  \n",
       "count  178.000000  \n",
       "mean     0.938202  \n",
       "std      0.775035  \n",
       "min      0.000000  \n",
       "25%      0.000000  \n",
       "50%      1.000000  \n",
       "75%      2.000000  \n",
       "max      2.000000  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Fit everything together nicely into one dataframe after scaling in the previous step.\n",
    "\n",
    "data1 = pd.DataFrame(data= np.c_[dataset['data'], dataset['target']],\n",
    "                     columns= dataset['feature_names'] + ['target'])                     \n",
    "\n",
    "data1.describe()\n",
    "\n",
    "# Note that the descriptives don't actually tell us anything about our target variable, because it's a label.\n",
    "# You can see, however, that there doesn't seem to be any missing data (all counts 178) and you can also\n",
    "# use these statistics to get an idea of the distribution of your predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-nelson",
   "metadata": {},
   "source": [
    "We now want to fit a logistic regression model to predict the membership in a group.\n",
    "\n",
    "In this case, we choose a penalized regression model. You will remember that these models allow us to reduce the number of predictors (in this case 13 of them) to only contain the most informative ones.\n",
    "\n",
    "For Lasso regression, the choice of the penalty factor is quite important. Thinking back to the lecture slides, this is your tuning parameter lambda. It describes how much you want to penalize for any added parameter.\n",
    "\n",
    "- A lambda of 0 implies that all predictors should be kept and no penalty should be applied.\n",
    "- A lambda of infinity would consider no predictors at all and a maximum penalty. Usually, we restrict the parameter to max out at 1 though.\n",
    "- With an increasing lambda, we consider fewer predictors. This increases the bias of the model. High bias is related to underfitting your model.\n",
    "- With a decreasing lambda, we consider more predictors. This increases the variance of the model. High variance is related to overfitting.\n",
    "\n",
    "In order to choose an optimal lambda for our case, we run k-fold cross validation to tune it while accounting for the model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fixed-wellington",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: {'C': 0.4901}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "from numpy import arange\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "lasso_logistic_model = LogisticRegression(\n",
    "    penalty='l1', # L1 penalty refers to Lasso regression (L2 would be ridge regression and 'elasticnet' elastic net)\n",
    "    solver='liblinear') # we choose this solver here because it's the fastest for small datasets\n",
    "\n",
    "\n",
    "# Let's now look for the optimal value of lambda:\n",
    "\n",
    "grid = dict() \n",
    "grid['C'] = arange(0.0001, 1, 0.01)\n",
    "\n",
    "# 5-fold cross validation, evaluating by model accuracy\n",
    "search = GridSearchCV(lasso_logistic_model, grid, scoring='accuracy', cv=5, refit=True)\n",
    "results = search.fit(x_train_scaled, y_train)\n",
    "\n",
    "print('Config: %s' % results.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7d0f1c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=LogisticRegression(penalty=&#x27;l1&#x27;, solver=&#x27;liblinear&#x27;),\n",
       "             param_grid={&#x27;C&#x27;: array([1.000e-04, 1.010e-02, 2.010e-02, 3.010e-02, 4.010e-02, 5.010e-02,\n",
       "       6.010e-02, 7.010e-02, 8.010e-02, 9.010e-02, 1.001e-01, 1.101e-01,\n",
       "       1.201e-01, 1.301e-01, 1.401e-01, 1.501e-01, 1.601e-01, 1.701e-01,\n",
       "       1.801e-01, 1.901e-01, 2.001e-01, 2.101e-01, 2.201e-01, 2.301e-01,\n",
       "       2.401e-...\n",
       "       6.601e-01, 6.701e-01, 6.801e-01, 6.901e-01, 7.001e-01, 7.101e-01,\n",
       "       7.201e-01, 7.301e-01, 7.401e-01, 7.501e-01, 7.601e-01, 7.701e-01,\n",
       "       7.801e-01, 7.901e-01, 8.001e-01, 8.101e-01, 8.201e-01, 8.301e-01,\n",
       "       8.401e-01, 8.501e-01, 8.601e-01, 8.701e-01, 8.801e-01, 8.901e-01,\n",
       "       9.001e-01, 9.101e-01, 9.201e-01, 9.301e-01, 9.401e-01, 9.501e-01,\n",
       "       9.601e-01, 9.701e-01, 9.801e-01, 9.901e-01])},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=LogisticRegression(penalty=&#x27;l1&#x27;, solver=&#x27;liblinear&#x27;),\n",
       "             param_grid={&#x27;C&#x27;: array([1.000e-04, 1.010e-02, 2.010e-02, 3.010e-02, 4.010e-02, 5.010e-02,\n",
       "       6.010e-02, 7.010e-02, 8.010e-02, 9.010e-02, 1.001e-01, 1.101e-01,\n",
       "       1.201e-01, 1.301e-01, 1.401e-01, 1.501e-01, 1.601e-01, 1.701e-01,\n",
       "       1.801e-01, 1.901e-01, 2.001e-01, 2.101e-01, 2.201e-01, 2.301e-01,\n",
       "       2.401e-...\n",
       "       6.601e-01, 6.701e-01, 6.801e-01, 6.901e-01, 7.001e-01, 7.101e-01,\n",
       "       7.201e-01, 7.301e-01, 7.401e-01, 7.501e-01, 7.601e-01, 7.701e-01,\n",
       "       7.801e-01, 7.901e-01, 8.001e-01, 8.101e-01, 8.201e-01, 8.301e-01,\n",
       "       8.401e-01, 8.501e-01, 8.601e-01, 8.701e-01, 8.801e-01, 8.901e-01,\n",
       "       9.001e-01, 9.101e-01, 9.201e-01, 9.301e-01, 9.401e-01, 9.501e-01,\n",
       "       9.601e-01, 9.701e-01, 9.801e-01, 9.901e-01])},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(penalty=&#x27;l1&#x27;, solver=&#x27;liblinear&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(penalty=&#x27;l1&#x27;, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=LogisticRegression(penalty='l1', solver='liblinear'),\n",
       "             param_grid={'C': array([1.000e-04, 1.010e-02, 2.010e-02, 3.010e-02, 4.010e-02, 5.010e-02,\n",
       "       6.010e-02, 7.010e-02, 8.010e-02, 9.010e-02, 1.001e-01, 1.101e-01,\n",
       "       1.201e-01, 1.301e-01, 1.401e-01, 1.501e-01, 1.601e-01, 1.701e-01,\n",
       "       1.801e-01, 1.901e-01, 2.001e-01, 2.101e-01, 2.201e-01, 2.301e-01,\n",
       "       2.401e-...\n",
       "       6.601e-01, 6.701e-01, 6.801e-01, 6.901e-01, 7.001e-01, 7.101e-01,\n",
       "       7.201e-01, 7.301e-01, 7.401e-01, 7.501e-01, 7.601e-01, 7.701e-01,\n",
       "       7.801e-01, 7.901e-01, 8.001e-01, 8.101e-01, 8.201e-01, 8.301e-01,\n",
       "       8.401e-01, 8.501e-01, 8.601e-01, 8.701e-01, 8.801e-01, 8.901e-01,\n",
       "       9.001e-01, 9.101e-01, 9.201e-01, 9.301e-01, 9.401e-01, 9.501e-01,\n",
       "       9.601e-01, 9.701e-01, 9.801e-01, 9.901e-01])},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-evaluation",
   "metadata": {},
   "source": [
    "We have identified our optimal lambda in this case to be 0.4901.\n",
    "\n",
    "But what is the actual impact of this penalization term on the results of our logistic regression?\n",
    "\n",
    "\n",
    "Let's visualize the coefficients of the lasso-logistic model and compare with those of the standard (non-penalized) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "strategic-wagon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients of the lasso logistic model: \n",
      "\n",
      " [[ 1.03450196  0.          0.56434198 -1.04122607  0.          0.\n",
      "   0.93442124  0.          0.          0.          0.          0.46094534\n",
      "   1.64085113]\n",
      " [-1.4105756  -0.08080792 -0.80041383  0.46451271  0.          0.\n",
      "   0.15585241  0.          0.29161264 -1.29587215  1.05645296  0.\n",
      "  -1.42021967]\n",
      " [ 0.          0.17382347  0.15587845  0.          0.          0.\n",
      "  -1.55535453  0.          0.          1.2964283  -0.79217463 -0.64661293\n",
      "   0.        ]]\n",
      "\n",
      "\n",
      " Coefficients of the traditional logistic model: \n",
      "\n",
      " [[ 4.07894388  1.40496807  2.6084901  -4.70856028  0.65505705  1.16054494\n",
      "   2.13844972  0.65351972  1.11618752  0.3218668  -0.1976088   3.00237699\n",
      "   4.26715433]\n",
      " [-6.59464113 -0.8653589  -5.73187583  4.49811726  0.26276841 -2.24041925\n",
      "   3.08249089  0.82525615  1.7858793  -4.29955537  5.63507782 -0.32811002\n",
      "  -6.42966757]\n",
      " [ 2.51569726 -0.53960917  3.12338573  0.21044302 -0.91782547  1.07987431\n",
      "  -5.22094061 -1.47877587 -2.90206682  3.97768857 -5.43746902 -2.67426698\n",
      "   2.16251324]]\n"
     ]
    }
   ],
   "source": [
    "# Coefficients of the lasso-logistic model\n",
    "lasso_model = LogisticRegression(\n",
    "    penalty='l1', # Lasso regression\n",
    "    solver='liblinear',\n",
    "    C = 0.4901).fit(x_train_scaled,y_train) #here is your new penalty term from before\n",
    "print(\"Coefficients of the lasso logistic model: \\n\\n\",lasso_model.coef_)\n",
    "\n",
    "# Coefficients of the traditional logistic model\n",
    "logistic_model = LogisticRegression( # use the same model\n",
    "    penalty='none').fit(x_train_scaled,y_train) # but without the penalty\n",
    "print(\"\\n\\n Coefficients of the traditional logistic model: \\n\\n\",logistic_model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f7880cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, 0, 1, 0, 1, 2, 1, 2, 0, 2, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,\n",
       "       1, 2, 2, 2, 1, 1, 1, 0, 0, 1, 2, 0, 0, 0, 2, 2, 1, 2, 0, 1, 1, 1,\n",
       "       2, 0, 1, 1, 2, 0, 1, 0, 0, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_model.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea26163",
   "metadata": {},
   "source": [
    "Note how the output from the scikit learn package are not as \"pretty\" as the ones we got last week from our statsmodel summary. Indeed, scitkit learn outputs don't have a summary attribute, which means no p-values to look at!\n",
    "\n",
    "In order to get those, we could use the statsmodel package. \n",
    "\n",
    "But today we're not that interested in p-value comparisons, so let's instead continue comparing predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "reliable-theater",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Predictions of the Lasso logistic model: \n",
      "\n",
      " [0 0 2 0 1 0 1 2 1 2 0 2 0 1 0 1 1 1 0 1 0 1 1 2 2 2 1 1 1 0 0 1 2 0 0 0 2\n",
      " 2 1 2 0 1 1 1 2 0 1 1 2 0 1 0 0 2]\n",
      "\n",
      "\n",
      " Predictions of the traditional logistic model: \n",
      "\n",
      " [0 0 2 0 1 0 1 2 1 2 0 2 0 1 0 1 1 1 0 1 0 1 1 2 2 2 1 1 1 0 0 1 2 0 0 0 2\n",
      " 2 1 2 0 1 1 2 2 0 1 1 2 0 1 0 0 2]\n"
     ]
    }
   ],
   "source": [
    "# Predictions with the lasso-logistic model\n",
    "predictions_tuned_model = search.predict(x_test_scaled) # get predictions for our test data\n",
    "# equivalent to: predictions_tuned_model = lasso_model.predict(x_test_scaled)\n",
    "#\n",
    "print(\"\\n\\n Predictions of the Lasso logistic model: \\n\\n\", predictions_tuned_model)\n",
    "\n",
    "# Predictions with the standard logistic model\n",
    "logictic_model = LogisticRegression(solver='liblinear').fit(x_train_scaled,y_train)\n",
    "\n",
    "predictions_non_tuned_model = logictic_model.predict(x_test_scaled)\n",
    "\n",
    "print(\"\\n\\n Predictions of the traditional logistic model: \\n\\n\", predictions_non_tuned_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-rotation",
   "metadata": {},
   "source": [
    "Let's evaluate the performance of each model by looking at their confusion matrices.\n",
    "\n",
    "In problems with a small number of classes, they can give you a good overview of correctly and incorrectly classified elements.\n",
    "\n",
    "We also compute the accuracy of both models based on the number of correctly/incorrectly classified instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "accepted-technique",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for the lasso-logistic model: \n",
      "[[19  0  0]\n",
      " [ 0 21  0]\n",
      " [ 0  0 14]]\n",
      "Confusion matrix for the traditional logistic model: \n",
      "[[19  0  0]\n",
      " [ 0 20  1]\n",
      " [ 0  0 14]]\n",
      "Accuracy lasso model: 1.0\n",
      "Accuracy traditional model: 0.9814814814814815\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "\n",
    "print(\"Confusion matrix for the lasso-logistic model: \\n\"+str(cm(y_test,predictions_tuned_model)))\n",
    "\n",
    "print(\"Confusion matrix for the traditional logistic model: \\n\"+str(cm(y_test,predictions_non_tuned_model)))\n",
    "\n",
    "print(\"Accuracy lasso model: \"+str(accuracy(y_test,predictions_tuned_model)))\n",
    "\n",
    "print(\"Accuracy traditional model: \"+str(accuracy(y_test,predictions_non_tuned_model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6631a837",
   "metadata": {},
   "source": [
    "From these outputs, we can see that our tuned lasso logistic model outperforms the traditional one.\n",
    "\n",
    "Discussion question: how significant should an improvement be to \"justify\" the additional work that has gone into the lasso regression model? Maybe there is a trade-off due to the additional computational costs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-cement",
   "metadata": {},
   "source": [
    "# Addressing imbalance while tuning a classification model \n",
    "\n",
    "We have seen in our dataset above that the implementation of a (penalized) logistic regression is fairly straightforward in cases in which the datasets are well-balanced.\n",
    "\n",
    "In many real-life scenarios you will encounter unbalanced datasets and you will have to do some pre-processing. Unbalanced datasets don't allow the algorithm to learn what determines group membership for all groups equally. This is a common problem in datasets where a specific class is rare, for example default in credit scoring or specific health conditions.\n",
    "\n",
    "In this example, we will use a dataset on breast cancer. You will understand that being able to accurately predict breast cancer occurrence is very important - think about the danger of making different types of errors in this case:\n",
    "\n",
    "- Someone does not have cancer, but is flagged as a positive case -> can cause distress\n",
    "- Someone does have cancer, but is flagged as a negative case -> can cause bodily harm\n",
    "\n",
    "\n",
    "In this exercise, we will use SMOTE (Synthetic Minority Oversampling Technique) to balance out classes and integrate it within the modelling cycle for tuning our lasso-logistic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "elegant-roller",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 569\n",
      "\n",
      "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      "    :Attribute Information:\n",
      "        - radius (mean of distances from center to points on the perimeter)\n",
      "        - texture (standard deviation of gray-scale values)\n",
      "        - perimeter\n",
      "        - area\n",
      "        - smoothness (local variation in radius lengths)\n",
      "        - compactness (perimeter^2 / area - 1.0)\n",
      "        - concavity (severity of concave portions of the contour)\n",
      "        - concave points (number of concave portions of the contour)\n",
      "        - symmetry\n",
      "        - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "        worst/largest values) of these features were computed for each image,\n",
      "        resulting in 30 features.  For instance, field 0 is Mean Radius, field\n",
      "        10 is Radius SE, field 20 is Worst Radius.\n",
      "\n",
      "        - class:\n",
      "                - WDBC-Malignant\n",
      "                - WDBC-Benign\n",
      "\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ===================================== ====== ======\n",
      "                                           Min    Max\n",
      "    ===================================== ====== ======\n",
      "    radius (mean):                        6.981  28.11\n",
      "    texture (mean):                       9.71   39.28\n",
      "    perimeter (mean):                     43.79  188.5\n",
      "    area (mean):                          143.5  2501.0\n",
      "    smoothness (mean):                    0.053  0.163\n",
      "    compactness (mean):                   0.019  0.345\n",
      "    concavity (mean):                     0.0    0.427\n",
      "    concave points (mean):                0.0    0.201\n",
      "    symmetry (mean):                      0.106  0.304\n",
      "    fractal dimension (mean):             0.05   0.097\n",
      "    radius (standard error):              0.112  2.873\n",
      "    texture (standard error):             0.36   4.885\n",
      "    perimeter (standard error):           0.757  21.98\n",
      "    area (standard error):                6.802  542.2\n",
      "    smoothness (standard error):          0.002  0.031\n",
      "    compactness (standard error):         0.002  0.135\n",
      "    concavity (standard error):           0.0    0.396\n",
      "    concave points (standard error):      0.0    0.053\n",
      "    symmetry (standard error):            0.008  0.079\n",
      "    fractal dimension (standard error):   0.001  0.03\n",
      "    radius (worst):                       7.93   36.04\n",
      "    texture (worst):                      12.02  49.54\n",
      "    perimeter (worst):                    50.41  251.2\n",
      "    area (worst):                         185.2  4254.0\n",
      "    smoothness (worst):                   0.071  0.223\n",
      "    compactness (worst):                  0.027  1.058\n",
      "    concavity (worst):                    0.0    1.252\n",
      "    concave points (worst):               0.0    0.291\n",
      "    symmetry (worst):                     0.156  0.664\n",
      "    fractal dimension (worst):            0.055  0.208\n",
      "    ===================================== ====== ======\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      "    :Donor: Nick Street\n",
      "\n",
      "    :Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
      "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
      "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "     San Jose, CA, 1993.\n",
      "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
      "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
      "     July-August 1995.\n",
      "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
      "     163-171.\n"
     ]
    }
   ],
   "source": [
    "# remove comment symbol from the following if getting error:\n",
    "\n",
    "# !pip install imblearn\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html\n",
    "\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "import sklearn.datasets as datasets\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Load data set\n",
    "dataset = datasets.load_breast_cancer()\n",
    "X = dataset['data']\n",
    "y = dataset['target']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = tts(X,y,test_size=0.3,stratify=y,random_state=11)\n",
    "\n",
    "print(dataset.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a848e3",
   "metadata": {},
   "source": [
    "Have an initial look at the data. We have\n",
    "\n",
    "- Number of Instances: 569\n",
    "- Number of Attributes: 30 numeric\n",
    "\n",
    "Note the class distribution:\n",
    "\n",
    "- 212 - Malignant\n",
    "- 357 - Benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "organic-houston",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: {'classifier__C': 0.5201}\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline with SMOTE in it\n",
    "pipeline = imbpipeline(steps = [['smote', SMOTE(random_state=11)],\n",
    "                                ['scaler', StandardScaler()],\n",
    "                                ['classifier', LogisticRegression(random_state=11, # this shuffles our data\n",
    "                                                                  penalty='l1',\n",
    "                                                                  solver='liblinear')]])\n",
    "\n",
    "# The pipeline module from the imblearn package is very useful in this case. It allows us to automatically run\n",
    "# multiple pre-processing and analysis steps in sequence.\n",
    "\n",
    "\n",
    "# Grid for the shrinkage intensity, i.e. our penalty factor\n",
    "grid = dict() \n",
    "grid['classifier__C'] = arange(0.0001, 2, 0.01)\n",
    "\n",
    "# Setup stratified cross-validation\n",
    "stratified_kfold = StratifiedKFold(n_splits=5,\n",
    "                                   shuffle=True,\n",
    "                                   random_state=11) # note that we are setting the same random seed for all modelling steps\n",
    "\n",
    "# Run cross-validation using the AUC as scoring metric\n",
    "\n",
    "search = GridSearchCV(pipeline, grid, scoring='roc_auc', cv=stratified_kfold, refit=True)\n",
    "results = search.fit(X_train, y_train)\n",
    "print('Config: %s' % results.best_params_)\n",
    "\n",
    "# As a side note\n",
    "# Remember that AUC is a great tool for comparison -between- models. It should NOT be used on its own for \n",
    "# model evaluation, it's strictly a comparison tool. Here, we use it to compare between cross-validation runs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2948a3f",
   "metadata": {},
   "source": [
    "Our optimal lambda in this case is 0.5201.\n",
    "\n",
    "Let's look at what this model has built us as a pipeline. Use this to double check the order of elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fff659b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=11, shuffle=True),\n",
       "             estimator=Pipeline(steps=[(&#x27;smote&#x27;, SMOTE(random_state=11)),\n",
       "                                       (&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                       [&#x27;classifier&#x27;,\n",
       "                                        LogisticRegression(penalty=&#x27;l1&#x27;,\n",
       "                                                           random_state=11,\n",
       "                                                           solver=&#x27;liblinear&#x27;)]]),\n",
       "             param_grid={&#x27;classifier__C&#x27;: array([1.0000e-04, 1.0100e-02, 2.0100e-02, 3.0100e-02, 4.0100e-02,\n",
       "       5.0100e-02, 6.01...\n",
       "       1.7001e+00, 1.7101e+00, 1.7201e+00, 1.7301e+00, 1.7401e+00,\n",
       "       1.7501e+00, 1.7601e+00, 1.7701e+00, 1.7801e+00, 1.7901e+00,\n",
       "       1.8001e+00, 1.8101e+00, 1.8201e+00, 1.8301e+00, 1.8401e+00,\n",
       "       1.8501e+00, 1.8601e+00, 1.8701e+00, 1.8801e+00, 1.8901e+00,\n",
       "       1.9001e+00, 1.9101e+00, 1.9201e+00, 1.9301e+00, 1.9401e+00,\n",
       "       1.9501e+00, 1.9601e+00, 1.9701e+00, 1.9801e+00, 1.9901e+00])},\n",
       "             scoring=&#x27;roc_auc&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-32\" type=\"checkbox\" ><label for=\"sk-estimator-id-32\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=11, shuffle=True),\n",
       "             estimator=Pipeline(steps=[(&#x27;smote&#x27;, SMOTE(random_state=11)),\n",
       "                                       (&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                       [&#x27;classifier&#x27;,\n",
       "                                        LogisticRegression(penalty=&#x27;l1&#x27;,\n",
       "                                                           random_state=11,\n",
       "                                                           solver=&#x27;liblinear&#x27;)]]),\n",
       "             param_grid={&#x27;classifier__C&#x27;: array([1.0000e-04, 1.0100e-02, 2.0100e-02, 3.0100e-02, 4.0100e-02,\n",
       "       5.0100e-02, 6.01...\n",
       "       1.7001e+00, 1.7101e+00, 1.7201e+00, 1.7301e+00, 1.7401e+00,\n",
       "       1.7501e+00, 1.7601e+00, 1.7701e+00, 1.7801e+00, 1.7901e+00,\n",
       "       1.8001e+00, 1.8101e+00, 1.8201e+00, 1.8301e+00, 1.8401e+00,\n",
       "       1.8501e+00, 1.8601e+00, 1.8701e+00, 1.8801e+00, 1.8901e+00,\n",
       "       1.9001e+00, 1.9101e+00, 1.9201e+00, 1.9301e+00, 1.9401e+00,\n",
       "       1.9501e+00, 1.9601e+00, 1.9701e+00, 1.9801e+00, 1.9901e+00])},\n",
       "             scoring=&#x27;roc_auc&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-33\" type=\"checkbox\" ><label for=\"sk-estimator-id-33\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;smote&#x27;, SMOTE(random_state=11)), (&#x27;scaler&#x27;, StandardScaler()),\n",
       "                [&#x27;classifier&#x27;,\n",
       "                 LogisticRegression(penalty=&#x27;l1&#x27;, random_state=11,\n",
       "                                    solver=&#x27;liblinear&#x27;)]])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-34\" type=\"checkbox\" ><label for=\"sk-estimator-id-34\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SMOTE</label><div class=\"sk-toggleable__content\"><pre>SMOTE(random_state=11)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-35\" type=\"checkbox\" ><label for=\"sk-estimator-id-35\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-36\" type=\"checkbox\" ><label for=\"sk-estimator-id-36\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(penalty=&#x27;l1&#x27;, random_state=11, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=11, shuffle=True),\n",
       "             estimator=Pipeline(steps=[('smote', SMOTE(random_state=11)),\n",
       "                                       ('scaler', StandardScaler()),\n",
       "                                       ['classifier',\n",
       "                                        LogisticRegression(penalty='l1',\n",
       "                                                           random_state=11,\n",
       "                                                           solver='liblinear')]]),\n",
       "             param_grid={'classifier__C': array([1.0000e-04, 1.0100e-02, 2.0100e-02, 3.0100e-02, 4.0100e-02,\n",
       "       5.0100e-02, 6.01...\n",
       "       1.7001e+00, 1.7101e+00, 1.7201e+00, 1.7301e+00, 1.7401e+00,\n",
       "       1.7501e+00, 1.7601e+00, 1.7701e+00, 1.7801e+00, 1.7901e+00,\n",
       "       1.8001e+00, 1.8101e+00, 1.8201e+00, 1.8301e+00, 1.8401e+00,\n",
       "       1.8501e+00, 1.8601e+00, 1.8701e+00, 1.8801e+00, 1.8901e+00,\n",
       "       1.9001e+00, 1.9101e+00, 1.9201e+00, 1.9301e+00, 1.9401e+00,\n",
       "       1.9501e+00, 1.9601e+00, 1.9701e+00, 1.9801e+00, 1.9901e+00])},\n",
       "             scoring='roc_auc')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "democratic-confirmation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score: 0.9978344827586207\n",
      "Test score: 0.981892523364486\n"
     ]
    }
   ],
   "source": [
    "cv_score = search.best_score_ # best AUC score over all cross-validation runs\n",
    "\n",
    "test_score = search.score(X_test, y_test) # AUC score over test data\n",
    "\n",
    "print(f'Cross-validation score: {cv_score}\\nTest score: {test_score}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
