{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at some numeric data we can cluster. First, we generate data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "np.random.seed(5)\n",
    "\n",
    "# scikit-learn contains a specific function to create data\n",
    "# It creates a tuple with X and y matrices\n",
    "# We create a dataset with 2 independent variables, and a dependent variable with 4 levels/variables\n",
    "data = make_classification(n_samples=100, n_features=2, \n",
    "                              n_informative=2, n_redundant=0, n_repeated=0, n_classes=4, \n",
    "                              n_clusters_per_class=1, \n",
    "                              class_sep=5.0, shuffle=True, random_state=6)\n",
    "\n",
    "# Here, we store the X data in a pandas dataframe\n",
    "X = pd.DataFrame(data = data[0], columns = ['var1','var2'])\n",
    "print(X.head())\n",
    "\n",
    "plt.scatter(X['var1'], X['var2'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering them is straightforward with scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# First, we create the model with parameters\n",
    "k_m = KMeans(n_clusters = 5, init=\"random\")\n",
    "\n",
    "# Next, we fit the data and predict (not really applicable here), but assign cluster labels\n",
    "k_m.fit_predict(X)\n",
    "\n",
    "# The k_m object stores both the labels and clusters, which can be accessed like this:\n",
    "print(k_m.labels_)\n",
    "print(k_m.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now visualise the clusters along the dimensions of these two variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a dictionary to map our labels (clusters) to colours\n",
    "colours = {0 : 'black', 1 : 'red', 2: 'green', 3: 'blue', 4: 'yellow'}\n",
    "\n",
    "# We loop the rows in the dataset (which returns a tuple of an index and observation)\n",
    "# as well as the cluster labels and colour the observations accordingly\n",
    "for (index, obs), label in zip(X.iterrows(), k_m.labels_):\n",
    "    plt.scatter(obs['var1'], obs['var2'], color = colours[label])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have more clusters than we actually need. The one in the lower left corner is split in two unnecessarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_m = KMeans(n_clusters = 2, init=\"random\")\n",
    "k_m.fit_predict(X)\n",
    "for (index, obs), label in zip(X.iterrows(), k_m.labels_):\n",
    "    plt.scatter(obs['var1'], obs['var2'], color = colours[label])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, two clusters would not be enough. The outcome seems arbitrary, as we could have also put the lower left corner in the red cluster, and the upper right in the black one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A different dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at data that is closer (parameter ```class_sep``` is lower) together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = make_classification(n_samples=100, n_features=2, \n",
    "                              n_informative=2, n_redundant=0, n_repeated=0, n_classes=4, \n",
    "                              n_clusters_per_class=1, \n",
    "                              class_sep=0.4, shuffle=True, random_state=6)\n",
    "\n",
    "X = pd.DataFrame(data = data[0], columns = ['var1','var2'])\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 1) \n",
    "k_m = KMeans(n_clusters = 2, init=\"random\")\n",
    "k_m.fit_predict(X)\n",
    "for (index, obs), label in zip(X.iterrows(), k_m.labels_):\n",
    "    plt.scatter(obs['var1'], obs['var2'], color = colours[label])\n",
    "plt.title('2 clusters')\n",
    "    \n",
    "plt.subplot(1, 2, 2) \n",
    "k_m = KMeans(n_clusters = 5, init=\"random\")\n",
    "k_m.fit_predict(X)\n",
    "for (index, obs), label in zip(X.iterrows(), k_m.labels_):\n",
    "    plt.scatter(obs['var1'], obs['var2'], color = colours[label])\n",
    "plt.title('5 clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no clearly delineated clusters. Nevertheless, each cluster has its own region which might make sense in some way. For example, the blue ones might represent people that spend very little (var 1 is low) on fair-priced products (medium value for var 2), while the green ones spend average (medium value for var 1), but only on cheap products (values for var 2 is low). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining the number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining the number of clusters to use can be done in numerous ways, as explained earlier. Later, you will learn about the sum of squared errors (SSE) which can be used to quantify the distance between points in a cluster and its centroid. This can be used to plot the decrease over the number of clusters to see whether there is still extra error reduction by introducing more clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = make_classification(n_samples=100, n_features=2, \n",
    "                              n_informative=2, n_redundant=0, n_repeated=0, n_classes=4, \n",
    "                              n_clusters_per_class=1, \n",
    "                              class_sep=4, shuffle=True, random_state=6)\n",
    "X = pd.DataFrame(data = data[0], columns = ['var1','var2'])\n",
    "\n",
    "sse = {}\n",
    "for k in range(1, 10):\n",
    "    kmeans = KMeans(n_clusters=k, max_iter=20).fit(X)\n",
    "    \n",
    "    # Inertia is the distance of each observation to its closest centroid\n",
    "    sse[k] = kmeans.inertia_ \n",
    "plt.figure()\n",
    "plt.plot(list(sse.keys()), list(sse.values()))\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you notice how the SSE does not change much after 4 clusters? There is a clear point in the curve where it starts to flatten. Finding this point is called the elbow method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = make_classification(n_samples=100, n_features=2, \n",
    "                              n_informative=2, n_redundant=0, n_repeated=0, n_classes=2, \n",
    "                              n_clusters_per_class=1, \n",
    "                              class_sep=0.4, shuffle=True, random_state=6)\n",
    "X = pd.DataFrame(data = data[0], columns = ['var1','var2'])\n",
    "\n",
    "sse = {}\n",
    "for k in range(1, 10):\n",
    "    kmeans = KMeans(n_clusters=k, max_iter=20).fit(X)\n",
    "    sse[k] = kmeans.inertia_ \n",
    "plt.figure()\n",
    "plt.plot(list(sse.keys()), list(sse.values()))\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, with only two generated clusters (```n_classes = 2```), there is a less pronounced elbow point, as the separation (```class_sep```) is low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a side note, it might be a good time to learn about colour maps as well. They come in handy when we need to convert numbers into colours for visualisation purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Creating data\n",
    "data = make_classification(n_samples=100, n_features=2, \n",
    "                              n_informative=2, n_redundant=0, n_repeated=0, n_classes=4, \n",
    "                              n_clusters_per_class=1, \n",
    "                              class_sep=4, shuffle=True, random_state=6)\n",
    "\n",
    "X = pd.DataFrame(data = data[0], columns = ['var1','var2'])\n",
    "\n",
    "# 4-means clustering\n",
    "no_clusters = 4\n",
    "k_m = KMeans(n_clusters = no_clusters, init=\"random\")\n",
    "k_m.fit_predict(X)\n",
    "\n",
    "# We normalize the range of our labels so they are mapped to colour codes\n",
    "norm = mpl.colors.Normalize(vmin=0, vmax= no_clusters)\n",
    "\n",
    "# We import a colour map, in this case with 'hot'/reddish colours\n",
    "cmap = cm.hot\n",
    "\n",
    "# This object allows us to map our normalised values along the colour map\n",
    "m = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "# Now, we can apply the same loop, but using the ScalarMappable object:\n",
    "for (index, obs), label in zip(X.iterrows(), k_m.labels_):\n",
    "    plt.scatter(obs['var1'], obs['var2'], color = m.to_rgba(label))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dendogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Creating data\n",
    "data = make_classification(n_samples=100, n_features=2, \n",
    "                              n_informative=2, n_redundant=0, n_repeated=0, n_classes=4, \n",
    "                              n_clusters_per_class=1, \n",
    "                              class_sep=4, shuffle=True, random_state=6)\n",
    "\n",
    "X = pd.DataFrame(data = data[0], columns = ['var1','var2'])\n",
    "\n",
    "# dendogram \n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
    "                                      counts]).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "\n",
    "\n",
    "# setting distance_threshold=0 ensures we compute the full tree.\n",
    "model = AgglomerativeClustering(distance_threshold=0, n_clusters=None)\n",
    "\n",
    "model = model.fit(X)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "# plot the top three levels of the dendrogram\n",
    "plot_dendrogram(model, truncate_mode='level', p=3)\n",
    "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
